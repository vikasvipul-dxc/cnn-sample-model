ğŸ±ğŸ¶ CNN Image Classification â€“ Custom CNN vs Transfer Learning

Course: AIMLCZG511 â€“ Deep Neural Networks
Program: BITS Pilani WILP
Task: Compare a custom CNN with a transfer learning model

ğŸ“Œ Project Summary

This project implements and evaluates two approaches for image classification on the Cats vs Dogs dataset:

Custom CNN built from scratch

Transfer Learning using a pretrained CNN backbone

The goal is to compare:

Model performance

Loss convergence

Computational cost

Impact of Global Average Pooling (GAP)

ğŸ—‚ Dataset

Name: Cats vs Dogs

Classes: 2 (Cat, Dog)

Train/Test Split: 90/10

Image Size: 224 Ã— 224

ğŸ§  Models
ğŸ”¹ Custom CNN

Conv2D layers: 3

MaxPooling layers

Global Average Pooling (GAP)

Dropout regularization

Softmax output

ğŸ”¹ Transfer Learning Model

Pretrained CNN backbone (frozen layers)

Global Average Pooling

Custom classification head

Fine-tuned on dataset

âš™ï¸ Training Configuration
Parameter	Custom CNN	Transfer Learning
Optimizer	Adam	Adam
Loss	Sparse Categorical Crossentropy	Same
Epochs	20â€“30	5â€“10
Batch Size	32	32
ğŸ“Š Results
ğŸ§ª Custom CNN
Metric	Value
Accuracy	0.917
Precision	0.917
Recall	0.917
F1 Score	0.917

Loss Reduction:

(
0.6525
âˆ’
0.1522
)
/
0.6525
Ã—
100
=
76.7
%
(0.6525âˆ’0.1522)/0.6525Ã—100=76.7%

Training Time: ~1260 seconds
Parameters: 94,402

ğŸš€ Transfer Learning
Metric	Value
Accuracy	~0.713
Precision	~0.713
Recall	~0.713
F1 Score	~0.713

Training Time: ~680 seconds
Parameters: 266,626

ğŸ” Key Insights

Custom CNN outperformed transfer learning by ~20% accuracy.

GAP reduced parameters and helped prevent overfitting.

Transfer learning trained faster but was less effective for this dataset.

Custom CNN learned dataset-specific features more effectively.

ğŸ“ˆ Evaluation Metrics

Accuracy

Precision (macro)

Recall (macro)

F1-score (macro)

ğŸ§© Assignment Requirements Met

âœ” Custom CNN with GAP
âœ” Transfer learning with frozen backbone
âœ” Loss convergence tracked
âœ” All 4 metrics computed
âœ” Comparative analysis provided
âœ” JSON results output generated

â–¶ï¸ How to Run
1. Open notebook (.ipynb)
2. Restart & Run All
3. Ensure outputs and final JSON results appear

ğŸ¯ Conclusion

A well-designed custom CNN can outperform transfer learning when sufficient task-specific data is available, though transfer learning remains computationally efficient.
